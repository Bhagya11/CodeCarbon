{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b21de-5f91-4896-a3e3-bfa376b00851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:45:27] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 14:45:27] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:45:27] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 14:45:30] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 14:45:30] CPU Model on constant consumption mode: 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz\n",
      "[codecarbon WARNING @ 14:45:30] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 14:45:30] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:45:30] No GPU found.\n",
      "[codecarbon INFO @ 14:45:30] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 14:45:30] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:45:30]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 14:45:30]   Python version: 3.11.1\n",
      "[codecarbon INFO @ 14:45:30]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 14:45:30]   Available RAM : 11.747 GB\n",
      "[codecarbon INFO @ 14:45:30]   CPU count: 8 thread(s) in 8 physical CPU(s)\n",
      "[codecarbon INFO @ 14:45:30]   CPU model: 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz\n",
      "[codecarbon INFO @ 14:45:30]   GPU count: None\n",
      "[codecarbon INFO @ 14:45:30]   GPU model: None\n",
      "[codecarbon INFO @ 14:45:34] Emissions data (if any) will be saved to file c:\\Users\\chand\\Desktop\\Projects\\vitgnn\\emissions.csv\n",
      "[codecarbon INFO @ 14:45:50] Energy consumed for RAM : 0.000042 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 14:45:51] Delta energy consumed for CPU with constant : 0.000536 kWh, power : 112.0 W\n",
      "[codecarbon INFO @ 14:45:51] Energy consumed for All CPU : 0.000536 kWh\n",
      "[codecarbon INFO @ 14:45:51] 0.000578 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:46:04] Energy consumed for RAM : 0.000078 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 14:46:04] Delta energy consumed for CPU with constant : 0.000408 kWh, power : 112.0 W\n",
      "[codecarbon INFO @ 14:46:04] Energy consumed for All CPU : 0.000944 kWh\n",
      "[codecarbon INFO @ 14:46:04] 0.001021 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "from codecarbon import track_emissions\n",
    "import psutil\n",
    "import time\n",
    "import pandas as pd\n",
    "from contextlib import AbstractContextManager\n",
    "\n",
    "# Custom context manager class for resource monitoring\n",
    "class ResourceMonitor(AbstractContextManager):\n",
    "    def __init__(self, module_name):\n",
    "        self.module_name = module_name\n",
    "        self.metrics = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.start_cpu = psutil.cpu_percent(interval=0.1)  # Small interval for better accuracy\n",
    "        self.start_mem = psutil.Process().memory_info().rss / 1024**2  # MB\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        time.sleep(0.01)  # Small delay to ensure resource usage is captured\n",
    "        end_time = time.time()\n",
    "        end_cpu = psutil.cpu_percent(interval=0.1)\n",
    "        end_mem = psutil.Process().memory_info().rss / 1024**2  # MB\n",
    "        self.metrics = {\n",
    "            'module': self.module_name,\n",
    "            'cpu_usage_percent': max((end_cpu + self.start_cpu) / 2, 0.0),  # Avoid negative values\n",
    "            'memory_usage_mb': max(end_mem - self.start_mem, 0.0)  # Avoid negative values\n",
    "        }\n",
    "        print(f\"Debug: {self.module_name} metrics - CPU: {self.metrics['cpu_usage_percent']:.2f}%, RAM: {self.metrics['memory_usage_mb']:.2f} MB\")\n",
    "        return False  # Allow exceptions to propagate\n",
    "\n",
    "    def get_metrics(self):\n",
    "        if self.metrics is None:\n",
    "            print(f\"Warning: No metrics collected for {self.module_name}\")\n",
    "            return {'module': self.module_name, 'cpu_usage_percent': 0.0, 'memory_usage_mb': 0.0}\n",
    "        return self.metrics\n",
    "\n",
    "# Preprocess sMRI\n",
    "def preprocess_smri_slices(smri_data):\n",
    "    scaler = StandardScaler()\n",
    "    N, V, C, H, W = smri_data.shape\n",
    "    processed_slices = []\n",
    "    for view in range(V):  # Axial, coronal, sagittal\n",
    "        slices = smri_data[:, view, 0, :, :]\n",
    "        slices = scaler.fit_transform(slices.reshape(N, -1)).reshape(N, H, W)\n",
    "        processed_slices.append(torch.tensor(slices, dtype=torch.float32).unsqueeze(1))\n",
    "    return torch.stack(processed_slices, dim=1)\n",
    "\n",
    "# Preprocess phenotype data\n",
    "def preprocess_phenotype_data(phenotype_data, feature_names):\n",
    "    scaler = StandardScaler()\n",
    "    phenotype_data = scaler.fit_transform(phenotype_data)\n",
    "    graph_data_list = []\n",
    "    for sample in phenotype_data:\n",
    "        x = torch.tensor(sample, dtype=torch.float32).reshape(1, -1)\n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        graph_data = Data(x=x, edge_index=edge_index)\n",
    "        graph_data_list.append(graph_data)\n",
    "    return graph_data_list\n",
    "\n",
    "# Multi-view ViT\n",
    "class MultiViewViT(nn.Module):\n",
    "    def __init__(self, image_size=128, patch_size=16, num_classes=768):\n",
    "        super(MultiViewViT, self).__init__()\n",
    "        vit_config = ViTConfig(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            hidden_size=256,\n",
    "            num_attention_heads=8,\n",
    "            num_channels=1\n",
    "        )\n",
    "        self.vits = nn.ModuleList([ViTModel(vit_config) for _ in range(3)])\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=256*3, num_heads=8)\n",
    "        self.fc = nn.Linear(256*3, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[2] != 1:\n",
    "            raise ValueError(f\"Expected 1 channel, got {x.shape[2]}\")\n",
    "        view_features = []\n",
    "        for i, vit in enumerate(self.vits):\n",
    "            view_x = x[:, i, :, :, :]\n",
    "            vit_out = vit(view_x).last_hidden_state[:, 0, :]\n",
    "            view_features.append(vit_out)\n",
    "        view_features = torch.cat(view_features, dim=-1)\n",
    "        view_features = view_features.unsqueeze(0)\n",
    "        attn_output, _ = self.attention(view_features, view_features, view_features)\n",
    "        attn_output = attn_output.squeeze(0)\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "# GNN\n",
    "class PhenotypeGNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, output_dim=256):\n",
    "        super(PhenotypeGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "        self.fc = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    def forward(self, data_list):\n",
    "        batch_features = []\n",
    "        for data in data_list:\n",
    "            x, edge_index = data.x, data.edge_index\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = F.relu(self.conv2(x, edge_index))\n",
    "            x = self.fc(x.squeeze(0))\n",
    "            batch_features.append(x)\n",
    "        return torch.stack(batch_features, dim=0)\n",
    "\n",
    "# Cross-Modal Transformer\n",
    "class CrossModalTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=8):\n",
    "        super(CrossModalTransformer, self).__init__()\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc_phenotype = nn.Linear(256, embed_dim)\n",
    "\n",
    "    def forward(self, smri_features, phenotype_features):\n",
    "        phenotype_features = self.fc_phenotype(phenotype_features)\n",
    "        combined = torch.stack([smri_features, phenotype_features], dim=0)\n",
    "        attn_output, _ = self.cross_attention(combined, combined, combined)\n",
    "        fused = self.norm(attn_output[0] + attn_output[1])\n",
    "        return fused\n",
    "\n",
    "# Ensemble with Uncertainty\n",
    "class NeuralEnsemble(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=128, num_classes=2, num_estimators=5):\n",
    "        super(NeuralEnsemble, self).__init__()\n",
    "        self.estimators = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_dim, num_classes)\n",
    "            ) for _ in range(num_estimators)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        outputs = []\n",
    "        for estimator in self.estimators:\n",
    "            estimator.train(training)\n",
    "            outputs.append(estimator(x))\n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        mean_output = torch.mean(outputs, dim=0)\n",
    "        uncertainty = torch.std(outputs, dim=0).mean(dim=-1)\n",
    "        return mean_output, uncertainty\n",
    "\n",
    "# Complete Model\n",
    "class TransGraphASD(nn.Module):\n",
    "    def __init__(self, image_size=128, patch_size=16, phenotype_features=10):\n",
    "        super(TransGraphASD, self).__init__()\n",
    "        self.mv_vit = MultiViewViT(image_size, patch_size)\n",
    "        self.gnn = PhenotypeGNN(num_features=phenotype_features)\n",
    "        self.cmt = CrossModalTransformer()\n",
    "        self.ensemble = NeuralEnsemble()\n",
    "\n",
    "    def forward(self, smri_data, phenotype_data, training=True):\n",
    "        batch_resources = []\n",
    "        \n",
    "        with ResourceMonitor('MultiViewViT') as smri_ctx:\n",
    "            smri_features = self.mv_vit(smri_data)\n",
    "            batch_resources.append(smri_ctx.get_metrics())\n",
    "        \n",
    "        with ResourceMonitor('PhenotypeGNN') as gnn_ctx:\n",
    "            phenotype_features = self.gnn(phenotype_data)\n",
    "            batch_resources.append(gnn_ctx.get_metrics())\n",
    "        \n",
    "        with ResourceMonitor('CrossModalTransformer') as cmt_ctx:\n",
    "            fused_features = self.cmt(smri_features, phenotype_features)\n",
    "            batch_resources.append(cmt_ctx.get_metrics())\n",
    "        \n",
    "        with ResourceMonitor('NeuralEnsemble') as ensemble_ctx:\n",
    "            output, uncertainty = self.ensemble(fused_features, training)\n",
    "            batch_resources.append(ensemble_ctx.get_metrics())\n",
    "        \n",
    "        return output, uncertainty, batch_resources\n",
    "\n",
    "# Contrastive Loss\n",
    "def contrastive_loss_fn(smri_features, phenotype_features):\n",
    "    assert smri_features.shape[1] == phenotype_features.shape[1]\n",
    "    similarity = F.cosine_similarity(smri_features, phenotype_features, dim=-1)\n",
    "    return -torch.mean(similarity)\n",
    "\n",
    "# Function to print resource utilization summary\n",
    "def print_resource_summary(resource_logs, phase=\"Training\"):\n",
    "    if not resource_logs:\n",
    "        print(f\"\\n{phase} Resource Utilization Summary: No data available\")\n",
    "        return\n",
    "    print(f\"\\n{phase} Resource Utilization Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    resource_df = pd.DataFrame([r for r in resource_logs if r and 'module' in r])\n",
    "    if resource_df.empty or 'module' not in resource_df.columns:\n",
    "        print(\"Error: No valid resource logs with 'module' key\")\n",
    "        return\n",
    "    for module in resource_df['module'].unique():\n",
    "        module_data = resource_df[resource_df['module'] == module]\n",
    "        avg_cpu = module_data['cpu_usage_percent'].mean()\n",
    "        avg_mem = module_data['memory_usage_mb'].mean()\n",
    "        print(f\"{module}:\")\n",
    "        print(f\"  Average CPU Usage: {avg_cpu:.2f}%\")\n",
    "        print(f\"  Average RAM Usage: {avg_mem:.2f} MB\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Training Function with Resource Monitoring\n",
    "@track_emissions(project_name=\"TransGraph-ASD Training\", country_iso_code=\"IND\")\n",
    "def train_model(model, smri_data, phenotype_data, labels, num_epochs=1, batch_size=64):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    resource_logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_resources = []\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i in range(0, len(smri_data), batch_size):\n",
    "            batch_smri = smri_data[i:i+batch_size]\n",
    "            batch_phenotype = phenotype_data[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _, batch_resources = model(batch_smri, batch_phenotype)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "\n",
    "            smri_features = model.mv_vit(batch_smri)\n",
    "            phenotype_features = model.gnn(batch_phenotype)\n",
    "            with torch.no_grad():\n",
    "                aligned_phenotype_features = model.cmt.fc_phenotype(phenotype_features)\n",
    "            contrastive_loss = contrastive_loss_fn(smri_features, aligned_phenotype_features)\n",
    "            total_loss = loss + 0.1 * contrastive_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            for resource in batch_resources:\n",
    "                if resource and 'module' in resource:\n",
    "                    resource['epoch'] = epoch + 1\n",
    "                    resource['batch'] = i // batch_size + 1\n",
    "                    epoch_resources.append(resource)\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid resource entry in batch {i // batch_size + 1}\")\n",
    "\n",
    "        avg_loss = total_loss.item() / (len(smri_data) // batch_size)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        print_resource_summary(epoch_resources, phase=f\"Training Epoch {epoch+1}\")\n",
    "        resource_logs.extend(epoch_resources)\n",
    "\n",
    "    pd.DataFrame(resource_logs).to_csv('resource_usage.csv', index=False)\n",
    "    return resource_logs\n",
    "\n",
    "# Inference with Uncertainty\n",
    "def predict_with_uncertainty(model, smri_data, phenotype_data, num_mc_samples=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    uncertainties = []\n",
    "    resource_logs = []\n",
    "\n",
    "    for i in range(num_mc_samples):\n",
    "        with torch.no_grad():\n",
    "            output, uncertainty, batch_resources = model(smri_data, phenotype_data, training=True)\n",
    "            predictions.append(F.softmax(output, dim=-1))\n",
    "            uncertainties.append(uncertainty)\n",
    "            for resource in batch_resources:\n",
    "                if resource and 'module' in resource:\n",
    "                    resource['sample'] = i + 1\n",
    "                    resource_logs.append(resource)\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid resource entry in MC sample {i + 1}\")\n",
    "\n",
    "    predictions = torch.mean(torch.stack(predictions), dim=0)\n",
    "    uncertainties = torch.mean(torch.stack(uncertainties), dim=0)\n",
    "\n",
    "    print_resource_summary(resource_logs, phase=\"Inference\")\n",
    "    return predictions, uncertainties, resource_logs\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    N, H, W, num_features = 100, 128, 128, 20  # Increased data size for measurable resource usage\n",
    "    smri_data = torch.randn(N, 3, 1, H, W)\n",
    "    phenotype_data = preprocess_phenotype_data(np.random.randn(N, num_features), [f\"f{i}\" for i in range(num_features)])\n",
    "    labels = torch.randint(0, 2, (N,))\n",
    "\n",
    "    model = TransGraphASD(image_size=H, patch_size=16, phenotype_features=num_features)\n",
    "    train_resource_logs = train_model(model, smri_data, phenotype_data, labels, num_epochs=1, batch_size=64)\n",
    "    predictions, uncertainties, inference_resource_logs = predict_with_uncertainty(model, smri_data, phenotype_data)\n",
    "    print(\"Predictions:\", predictions)\n",
    "    print(\"Uncertainties:\", uncertainties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92efc9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chand\\Desktop\\Projects\\vitgnn\\vitgnn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[codecarbon WARNING @ 09:05:45] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 09:05:45] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 09:05:45] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 09:05:49] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 09:05:49] CPU Model on constant consumption mode: 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz\n",
      "[codecarbon WARNING @ 09:05:49] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 09:05:49] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 09:05:49] No GPU found.\n",
      "[codecarbon INFO @ 09:05:49] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 09:05:49] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 09:05:49]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 09:05:49]   Python version: 3.11.1\n",
      "[codecarbon INFO @ 09:05:49]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 09:05:49]   Available RAM : 11.747 GB\n",
      "[codecarbon INFO @ 09:05:49]   CPU count: 8 thread(s) in 8 physical CPU(s)\n",
      "[codecarbon INFO @ 09:05:49]   CPU model: 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz\n",
      "[codecarbon INFO @ 09:05:49]   GPU count: None\n",
      "[codecarbon INFO @ 09:05:49]   GPU model: None\n",
      "[codecarbon INFO @ 09:05:50] Emissions data (if any) will be saved to file c:\\Users\\chand\\Desktop\\Projects\\vitgnn\\emissions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] MultiViewViT: start_cpu=67.9, end_cpu=98.2, start_mem=704.02, end_mem=1020.79, duration=1.39s\n",
      "[DEBUG] PhenotypeGNN: start_cpu=98.0, end_cpu=100.0, start_mem=1020.79, end_mem=1021.49, duration=0.16s\n",
      "[DEBUG] CrossModalTransformer: start_cpu=100.0, end_cpu=100.0, start_mem=1021.49, end_mem=1021.76, duration=0.12s\n",
      "[DEBUG] NeuralEnsemble: start_cpu=100.0, end_cpu=100.0, start_mem=1005.40, end_mem=985.32, duration=0.13s\n",
      "[DEBUG] MultiViewViT: start_cpu=98.2, end_cpu=100.0, start_mem=1244.87, end_mem=1277.12, duration=1.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:06:05] Energy consumed for RAM : 0.000042 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 09:06:05] Delta energy consumed for CPU with constant : 0.000470 kWh, power : 112.0 W\n",
      "[codecarbon INFO @ 09:06:05] Energy consumed for All CPU : 0.000470 kWh\n",
      "[codecarbon INFO @ 09:06:05] 0.000512 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] PhenotypeGNN: start_cpu=100.0, end_cpu=100.0, start_mem=1277.50, end_mem=1278.34, duration=0.15s\n",
      "[DEBUG] CrossModalTransformer: start_cpu=100.0, end_cpu=100.0, start_mem=1279.55, end_mem=1279.70, duration=0.12s\n",
      "[DEBUG] NeuralEnsemble: start_cpu=100.0, end_cpu=100.0, start_mem=1279.70, end_mem=1279.88, duration=0.12s\n",
      "[DEBUG] MultiViewViT: start_cpu=100.0, end_cpu=95.9, start_mem=1368.69, end_mem=1375.09, duration=1.15s\n",
      "[DEBUG] PhenotypeGNN: start_cpu=94.8, end_cpu=87.9, start_mem=1375.09, end_mem=1375.05, duration=0.12s\n",
      "[DEBUG] CrossModalTransformer: start_cpu=94.1, end_cpu=91.7, start_mem=1375.07, end_mem=1375.21, duration=0.12s\n",
      "[DEBUG] NeuralEnsemble: start_cpu=92.2, end_cpu=91.9, start_mem=1375.21, end_mem=1375.25, duration=0.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:06:20] Energy consumed for RAM : 0.000083 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 09:06:20] Delta energy consumed for CPU with constant : 0.000466 kWh, power : 112.0 W\n",
      "[codecarbon INFO @ 09:06:20] Energy consumed for All CPU : 0.000936 kWh\n",
      "[codecarbon INFO @ 09:06:20] 0.001019 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] MultiViewViT: start_cpu=96.5, end_cpu=89.7, start_mem=1371.57, end_mem=1377.95, duration=0.89s\n",
      "[DEBUG] PhenotypeGNN: start_cpu=86.5, end_cpu=92.5, start_mem=1377.96, end_mem=1377.98, duration=0.13s\n",
      "[DEBUG] CrossModalTransformer: start_cpu=98.0, end_cpu=100.0, start_mem=1377.99, end_mem=1378.06, duration=0.12s\n",
      "[DEBUG] NeuralEnsemble: start_cpu=100.0, end_cpu=96.8, start_mem=1378.08, end_mem=1378.12, duration=0.12s\n",
      "[DEBUG] MultiViewViT: start_cpu=91.4, end_cpu=96.5, start_mem=1372.50, end_mem=1378.57, duration=0.97s\n",
      "[DEBUG] PhenotypeGNN: start_cpu=66.7, end_cpu=76.3, start_mem=1378.57, end_mem=1378.64, duration=0.13s\n",
      "[DEBUG] CrossModalTransformer: start_cpu=74.5, end_cpu=69.4, start_mem=1378.65, end_mem=1378.72, duration=0.12s\n",
      "[DEBUG] NeuralEnsemble: start_cpu=72.7, end_cpu=78.3, start_mem=1378.73, end_mem=1378.77, duration=0.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:06:35] Energy consumed for RAM : 0.000125 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 09:06:35] Delta energy consumed for CPU with constant : 0.000466 kWh, power : 112.0 W\n",
      "[codecarbon INFO @ 09:06:35] Energy consumed for All CPU : 0.001402 kWh\n",
      "[codecarbon INFO @ 09:06:35] 0.001527 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] MultiViewViT: start_cpu=93.8, end_cpu=67.3, start_mem=1373.40, end_mem=1379.45, duration=0.92s\n",
      "[DEBUG] PhenotypeGNN: start_cpu=88.3, end_cpu=77.4, start_mem=1379.45, end_mem=1379.46, duration=0.13s\n",
      "[DEBUG] CrossModalTransformer: start_cpu=78.0, end_cpu=71.2, start_mem=1379.46, end_mem=1379.54, duration=0.12s\n",
      "[DEBUG] NeuralEnsemble: start_cpu=84.2, end_cpu=80.8, start_mem=1379.54, end_mem=1379.61, duration=0.12s\n",
      "[DEBUG] MultiViewViT: start_cpu=75.0, end_cpu=71.7, start_mem=1374.26, end_mem=1380.28, duration=0.86s\n",
      "[DEBUG] PhenotypeGNN: start_cpu=59.6, end_cpu=70.7, start_mem=1380.28, end_mem=1380.38, duration=0.14s\n",
      "[DEBUG] CrossModalTransformer: start_cpu=61.2, end_cpu=63.5, start_mem=1380.39, end_mem=1380.46, duration=0.12s\n",
      "[DEBUG] NeuralEnsemble: start_cpu=77.2, end_cpu=84.0, start_mem=1380.46, end_mem=1380.49, duration=0.12s\n",
      "[DEBUG] MultiViewViT: start_cpu=69.6, end_cpu=84.0, start_mem=1375.27, end_mem=1381.32, duration=0.88s\n",
      "[DEBUG] PhenotypeGNN: start_cpu=79.4, end_cpu=90.6, start_mem=1381.33, end_mem=1381.34, duration=0.13s\n",
      "[DEBUG] CrossModalTransformer: start_cpu=67.3, end_cpu=84.3, start_mem=1381.36, end_mem=1381.43, duration=0.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:06:50] Energy consumed for RAM : 0.000166 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 09:06:50] Delta energy consumed for CPU with constant : 0.000467 kWh, power : 112.0 W\n",
      "[codecarbon INFO @ 09:06:50] Energy consumed for All CPU : 0.001869 kWh\n",
      "[codecarbon INFO @ 09:06:50] 0.002035 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] NeuralEnsemble: start_cpu=100.0, end_cpu=88.9, start_mem=1381.43, end_mem=1381.50, duration=0.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:06:54] Energy consumed for RAM : 0.000179 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 09:06:54] Delta energy consumed for CPU with constant : 0.000147 kWh, power : 112.0 W\n",
      "[codecarbon INFO @ 09:06:54] Energy consumed for All CPU : 0.002016 kWh\n",
      "[codecarbon INFO @ 09:06:54] 0.002195 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6873\n",
      "\n",
      "Epoch 1 Resource Usage Summary\n",
      "----------------------------------------\n",
      "MultiViewViT: CPU Avg = 87.23%, RAM Avg = 48.25 MB\n",
      "PhenotypeGNN: CPU Avg = 85.54%, RAM Avg = 0.22 MB\n",
      "CrossModalTransformer: CPU Avg = 84.58%, RAM Avg = 0.12 MB\n",
      "NeuralEnsemble: CPU Avg = 90.44%, RAM Avg = 0.06 MB\n",
      "[DEBUG] MultiViewViT: start_cpu=74.5, end_cpu=93.9, start_mem=1375.38, end_mem=1045.81, duration=4.79s\n",
      "[DEBUG] PhenotypeGNN: start_cpu=95.0, end_cpu=89.8, start_mem=1045.84, end_mem=1045.85, duration=0.23s\n",
      "[DEBUG] CrossModalTransformer: start_cpu=88.1, end_cpu=89.7, start_mem=1045.85, end_mem=1045.62, duration=0.12s\n",
      "[DEBUG] NeuralEnsemble: start_cpu=96.2, end_cpu=100.0, start_mem=1045.62, end_mem=1045.71, duration=0.12s\n",
      "[DEBUG] MultiViewViT: start_cpu=100.0, end_cpu=77.2, start_mem=1045.71, end_mem=1045.66, duration=5.73s\n",
      "[DEBUG] PhenotypeGNN: start_cpu=74.5, end_cpu=98.1, start_mem=1045.66, end_mem=1045.67, duration=0.22s\n",
      "[DEBUG] CrossModalTransformer: start_cpu=77.2, end_cpu=86.7, start_mem=1045.67, end_mem=1045.67, duration=0.12s\n",
      "[DEBUG] NeuralEnsemble: start_cpu=73.5, end_cpu=84.0, start_mem=1045.67, end_mem=1045.67, duration=0.12s\n",
      "[DEBUG] MultiViewViT: start_cpu=66.0, end_cpu=82.5, start_mem=1045.67, end_mem=1045.67, duration=4.47s\n",
      "[DEBUG] PhenotypeGNN: start_cpu=83.7, end_cpu=72.9, start_mem=1045.67, end_mem=1045.69, duration=0.22s\n",
      "[DEBUG] CrossModalTransformer: start_cpu=93.9, end_cpu=73.5, start_mem=1045.69, end_mem=1045.69, duration=0.13s\n",
      "[DEBUG] NeuralEnsemble: start_cpu=86.2, end_cpu=75.0, start_mem=1045.69, end_mem=1045.69, duration=0.12s\n",
      "\n",
      "Inference Resource Usage Summary\n",
      "----------------------------------------\n",
      "MultiViewViT: CPU Avg = 82.35%, RAM Avg = 0.00 MB\n",
      "PhenotypeGNN: CPU Avg = 85.67%, RAM Avg = 0.01 MB\n",
      "CrossModalTransformer: CPU Avg = 84.85%, RAM Avg = 0.00 MB\n",
      "NeuralEnsemble: CPU Avg = 85.82%, RAM Avg = 0.03 MB\n",
      "\n",
      "Predictions: tensor([[0.6105, 0.3895],\n",
      "        [0.6262, 0.3738],\n",
      "        [0.6310, 0.3690],\n",
      "        [0.6471, 0.3529],\n",
      "        [0.6171, 0.3829],\n",
      "        [0.6178, 0.3822],\n",
      "        [0.6254, 0.3746],\n",
      "        [0.6160, 0.3840],\n",
      "        [0.6280, 0.3720],\n",
      "        [0.6329, 0.3671],\n",
      "        [0.6278, 0.3722],\n",
      "        [0.6196, 0.3804],\n",
      "        [0.6393, 0.3607],\n",
      "        [0.6335, 0.3665],\n",
      "        [0.6224, 0.3776],\n",
      "        [0.6417, 0.3583],\n",
      "        [0.6164, 0.3836],\n",
      "        [0.6287, 0.3713],\n",
      "        [0.6299, 0.3701],\n",
      "        [0.6189, 0.3811],\n",
      "        [0.6254, 0.3746],\n",
      "        [0.6034, 0.3966],\n",
      "        [0.6070, 0.3930],\n",
      "        [0.6387, 0.3613],\n",
      "        [0.6245, 0.3755],\n",
      "        [0.6319, 0.3681],\n",
      "        [0.6253, 0.3747],\n",
      "        [0.6276, 0.3724],\n",
      "        [0.5996, 0.4004],\n",
      "        [0.6103, 0.3897],\n",
      "        [0.6334, 0.3666],\n",
      "        [0.6471, 0.3529]])\n",
      "Uncertainties: tensor([0.2600, 0.1788, 0.1947, 0.2355, 0.2302, 0.2204, 0.2316, 0.2528, 0.2674,\n",
      "        0.2494, 0.2655, 0.1849, 0.1828, 0.2100, 0.1868, 0.1810, 0.1973, 0.1990,\n",
      "        0.1700, 0.2506, 0.1489, 0.1976, 0.2141, 0.1970, 0.2753, 0.2238, 0.2040,\n",
      "        0.2220, 0.2331, 0.2188, 0.1896, 0.2371])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "import psutil\n",
    "import time\n",
    "import pandas as pd\n",
    "from contextlib import AbstractContextManager\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# Resource Monitor\n",
    "class ResourceMonitor(AbstractContextManager):\n",
    "    def __init__(self, module_name):\n",
    "        self.module_name = module_name\n",
    "        self.metrics = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.start_cpu = psutil.cpu_percent(interval=0.1)\n",
    "        self.start_mem = psutil.Process().memory_info().rss / 1024**2\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        time.sleep(0.01)\n",
    "        end_time = time.time()\n",
    "        end_cpu = psutil.cpu_percent(interval=0.1)\n",
    "        end_mem = psutil.Process().memory_info().rss / 1024**2\n",
    "        self.metrics = {\n",
    "            'module': self.module_name,\n",
    "            'cpu_usage_percent': max((end_cpu + self.start_cpu) / 2, 0.0),\n",
    "            'memory_usage_mb': max(end_mem - self.start_mem, 0.0)\n",
    "        }\n",
    "        print(f\"[DEBUG] {self.module_name}: start_cpu={self.start_cpu}, end_cpu={end_cpu}, start_mem={self.start_mem:.2f}, end_mem={end_mem:.2f}, duration={end_time - self.start_time:.2f}s\")\n",
    "        return False\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return self.metrics or {'module': self.module_name, 'cpu_usage_percent': 0.0, 'memory_usage_mb': 0.0}\n",
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "def preprocess_smri_slices(smri_data):\n",
    "    scaler = StandardScaler()\n",
    "    N, V, C, H, W = smri_data.shape\n",
    "    processed = []\n",
    "    for view in range(V):\n",
    "        slices = smri_data[:, view, 0, :, :]\n",
    "        slices = scaler.fit_transform(slices.reshape(N, -1)).reshape(N, H, W)\n",
    "        processed.append(torch.tensor(slices, dtype=torch.float32).unsqueeze(1))\n",
    "    return torch.stack(processed, dim=1)\n",
    "\n",
    "def preprocess_phenotype_data(phenotype_data, feature_names):\n",
    "    scaler = StandardScaler()\n",
    "    phenotype_data = scaler.fit_transform(phenotype_data)\n",
    "    graph_data_list = []\n",
    "    for sample in phenotype_data:\n",
    "        x = torch.tensor(sample, dtype=torch.float32).reshape(1, -1)\n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        graph_data = Data(x=x, edge_index=edge_index)\n",
    "        graph_data_list.append(graph_data)\n",
    "    return graph_data_list\n",
    "\n",
    "# Models\n",
    "class MultiViewViT(nn.Module):\n",
    "    def __init__(self, image_size=128, patch_size=16, num_classes=768):\n",
    "        super().__init__()\n",
    "        vit_config = ViTConfig(\n",
    "            image_size=image_size, patch_size=patch_size,\n",
    "            hidden_size=256, num_attention_heads=8, num_channels=1\n",
    "        )\n",
    "        self.vits = nn.ModuleList([ViTModel(vit_config) for _ in range(3)])\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=256 * 3, num_heads=8)\n",
    "        self.fc = nn.Linear(256 * 3, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        view_features = []\n",
    "        for i, vit in enumerate(self.vits):\n",
    "            view_x = x[:, i, :, :, :]\n",
    "            vit_out = vit(view_x).last_hidden_state[:, 0, :]\n",
    "            view_features.append(vit_out)\n",
    "        combined = torch.cat(view_features, dim=-1).unsqueeze(0)\n",
    "        attn_out, _ = self.attn(combined, combined, combined)\n",
    "        return self.fc(attn_out.squeeze(0))\n",
    "\n",
    "class PhenotypeGNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "        self.fc = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    def forward(self, data_list):\n",
    "        batch = []\n",
    "        for data in data_list:\n",
    "            x = F.relu(self.conv1(data.x, data.edge_index))\n",
    "            x = F.relu(self.conv2(x, data.edge_index))\n",
    "            batch.append(self.fc(x.squeeze(0)))\n",
    "        return torch.stack(batch, dim=0)\n",
    "\n",
    "class CrossModalTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.fc_phenotype = nn.Linear(256, embed_dim)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, smri, phenotype):\n",
    "        pheno_proj = self.fc_phenotype(phenotype)\n",
    "        combined = torch.stack([smri, pheno_proj], dim=0)\n",
    "        out, _ = self.cross_attn(combined, combined, combined)\n",
    "        return self.norm(out[0] + out[1])\n",
    "\n",
    "class NeuralEnsemble(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=128, num_classes=2, num_estimators=5):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_dim, num_classes))\n",
    "            for _ in range(num_estimators)])\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        preds = []\n",
    "        for m in self.models:\n",
    "            m.train(training)\n",
    "            preds.append(m(x))\n",
    "        outputs = torch.stack(preds)\n",
    "        return outputs.mean(0), outputs.std(0).mean(-1)\n",
    "\n",
    "class TransGraphASD(nn.Module):\n",
    "    def __init__(self, image_size=128, patch_size=16, phenotype_features=10):\n",
    "        super().__init__()\n",
    "        self.mv_vit = MultiViewViT(image_size, patch_size)\n",
    "        self.gnn = PhenotypeGNN(phenotype_features)\n",
    "        self.cmt = CrossModalTransformer()\n",
    "        self.ensemble = NeuralEnsemble()\n",
    "\n",
    "    def forward(self, smri, pheno, training=True):\n",
    "        logs = []\n",
    "\n",
    "        with ResourceMonitor(\"MultiViewViT\") as m:\n",
    "            smri_feat = self.mv_vit(smri)\n",
    "        logs.append(m.get_metrics())\n",
    "\n",
    "        with ResourceMonitor(\"PhenotypeGNN\") as m:\n",
    "            pheno_feat = self.gnn(pheno)\n",
    "        logs.append(m.get_metrics())\n",
    "\n",
    "        with ResourceMonitor(\"CrossModalTransformer\") as m:\n",
    "            fused = self.cmt(smri_feat, pheno_feat)\n",
    "        logs.append(m.get_metrics())\n",
    "\n",
    "        with ResourceMonitor(\"NeuralEnsemble\") as m:\n",
    "            out, unc = self.ensemble(fused, training)\n",
    "        logs.append(m.get_metrics())\n",
    "\n",
    "        return out, unc, logs\n",
    "\n",
    "# Contrastive loss\n",
    "\n",
    "def contrastive_loss_fn(a, b):\n",
    "    return -torch.mean(F.cosine_similarity(a, b, dim=-1))\n",
    "\n",
    "# Training + inference\n",
    "\n",
    "def print_resource_summary(resource_logs, phase=\"Training\"):\n",
    "    print(f\"\\n{phase} Resource Usage Summary\\n\" + \"-\" * 40)\n",
    "    df = pd.DataFrame(resource_logs)\n",
    "    for mod in df['module'].unique():\n",
    "        sub = df[df['module'] == mod]\n",
    "        print(f\"{mod}: CPU Avg = {sub['cpu_usage_percent'].mean():.2f}%, RAM Avg = {sub['memory_usage_mb'].mean():.2f} MB\")\n",
    "\n",
    "\n",
    "def train_model(model, smri_data, pheno_data, labels, num_epochs=1, batch_size=4):\n",
    "    tracker = EmissionsTracker(project_name=\"TransGraph-ASD Training\", output_dir=\".\", output_file=\"emissions.csv\")\n",
    "    tracker.start()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    resource_logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(smri_data), batch_size):\n",
    "            batch_smri = smri_data[i:i+batch_size]\n",
    "            batch_pheno = pheno_data[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _, logs = model(batch_smri, batch_pheno)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "\n",
    "            smri_feat = model.mv_vit(batch_smri)\n",
    "            pheno_feat = model.gnn(batch_pheno)\n",
    "            pheno_proj = model.cmt.fc_phenotype(pheno_feat)\n",
    "            c_loss = contrastive_loss_fn(smri_feat, pheno_proj)\n",
    "            loss += 0.1 * c_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            resource_logs.extend(logs)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "        print_resource_summary(resource_logs, phase=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    tracker.stop()\n",
    "    return resource_logs\n",
    "\n",
    "\n",
    "def predict_with_uncertainty(model, smri_data, pheno_data, mc_samples=3):\n",
    "    model.eval()\n",
    "    preds, uncs, logs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(mc_samples):\n",
    "            p, u, l = model(smri_data, pheno_data, training=True)\n",
    "            preds.append(F.softmax(p, dim=-1))\n",
    "            uncs.append(u)\n",
    "            logs.extend(l)\n",
    "    print_resource_summary(logs, phase=\"Inference\")\n",
    "    return torch.mean(torch.stack(preds), dim=0), torch.mean(torch.stack(uncs), dim=0), logs\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    N, H, W, features = 32, 128, 128, 20\n",
    "    smri = torch.randn(N, 3, 1, H, W)\n",
    "    pheno = preprocess_phenotype_data(np.random.randn(N, features), [f\"f{i}\" for i in range(features)])\n",
    "    labels = torch.randint(0, 2, (N,))\n",
    "\n",
    "    model = TransGraphASD(image_size=H, patch_size=16, phenotype_features=features)\n",
    "    train_model(model, smri, pheno, labels)\n",
    "    preds, uncs, _ = predict_with_uncertainty(model, smri, pheno)\n",
    "    print(\"\\nPredictions:\", preds)\n",
    "    print(\"Uncertainties:\", uncs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23737fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae51b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitgnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
